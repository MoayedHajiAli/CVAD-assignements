{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oltMOCo4gkD0"
   },
   "source": [
    "# Visual Odometry (VO)\n",
    "\n",
    "In this assignment, you do not need. a GPU. You will use the pykitti module and KITTI odometry dataset.\n",
    "\n",
    "You can download the odometry data from [here](https://drive.google.com/file/d/1PJOUnM3nEwDpqiRvfqUnkNPJZpM4PKYV/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffiRr-EEgkD9"
   },
   "source": [
    "## Monocular VO with OpenCV on KITTI\n",
    "\n",
    "For each consecutive frame pair in the sequence, you will compute the relative pose between the frames and visualize it. You will use:\n",
    "\n",
    "* pykitti code similar to what you wrote in mvs part to load the seqeunce with ground-truth info. (Check out the [demo code](https://github.com/utiasSTARS/pykitti/blob/master/demos/demo_odometry.py))\n",
    "* OpenCV functions to compute and visualize the features and the essential matrix.\n",
    "\n",
    "Please follow these steps to complete the assignment:\n",
    "\n",
    "1. You can use the ORB Feature to do the feature matching:\n",
    "    `orb = cv2.ORB_create()` to create the ORB object\n",
    "    and then `orb.detectAndCompute()` to find the keypoints and descriptors on both frames\n",
    "\n",
    "2. You can use brute-force matcher to match ORB descriptors:\n",
    "    `bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)`\n",
    "\n",
    "3. After matching the descriptors, sort the matched keypoints.\n",
    "\n",
    "4. Draw matches on the two images using the `cv2.drawMatches()` function.\n",
    "\n",
    "5. Compute the essential matrix using the `cv2.findEssentialMat()` function. Note that you need the matching points and the instrinsics for this function. \n",
    "\n",
    "6. Extract the rotation and translation from the essential matrix using the `cv2.recoverPose()` function.\n",
    "\n",
    "7. Multiply the estimated rotation and translation with the previous rotation and translation. Initialize rotation to identity and translation to zeros on the first frame.\n",
    "\n",
    "8. Display the current image with the keypoints on it using the `cv2.drawKeypoints()` function.\n",
    "\n",
    "9. Update the previous rotation and translation as the current rotation and translation.\n",
    "\n",
    "10. Draw the estimated trajectory as blue and ground-truth trajectory as green. You can use the `cv2.circle()` function.\n",
    "\n",
    "\n",
    "You can create a video of your visualization of images and poses for the provided sequence.\n",
    "\n",
    "**Bonus**: Compute the absolute trajectory error between the estimated trajectory and the ground-truth trajectory. \n",
    "\n",
    "Some examples repositories that might be useful:\n",
    "* https://bitbucket.org/castacks/visual_odometry_tutorial/src/master/visual-odometry/\n",
    "* https://github.com/uoip/monoVO-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 1241.0\n",
    "height = 376.0\n",
    "fx, fy, cx, cy = [718.8560, 718.8560, 607.1928, 185.2157]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "trajMap = np.zeros((1000, 1000, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_2', 'image_3', 'calib.txt', 'times.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./KITTI_odometry/sequences/09/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_dir = './KITTI_odometry/sequences/09/image_2'\n",
    "paths = [path for path in os.listdir(path_dir) if path.endswith('.png')]\n",
    "paths = sorted(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gt trajectory\n",
    "gt_T = []\n",
    "with open('./KITTI_odometry/poses/09.txt') as f:\n",
    "    for line in f:\n",
    "        arr = list(map(float, line.split(' ')))\n",
    "        gt_T.append(np.array(arr).reshape(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "prev_img = cv2.imread(os.path.join(path_dir, paths[0]), 0)\n",
    "prev_R, prev_t = None, None\n",
    "prev_gt_R, prev_gt_t = None, None\n",
    "for i, path in enumerate(paths[1:]):\n",
    "    cur_img = cv2.imread(os.path.join(path_dir, path), 0)\n",
    "    \n",
    "    orb = cv2.ORB_create(nfeatures=6000)\n",
    "\n",
    "    kp1, des1 = orb.detectAndCompute(prev_img, None)\n",
    "    kp2, des2 = orb.detectAndCompute(cur_img, None)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "\n",
    "\n",
    "    img_matching = cv2.drawMatches(cur_img, kp1, prev_img, kp2, matches[0:100], None)\n",
    "\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, focal=fx, pp=(cx, cy), method=cv2.RANSAC, prob=0.999, threshold=1)\n",
    "    pts1 = pts1[mask.ravel() == 1]\n",
    "    pts2 = pts2[mask.ravel() == 1]\n",
    "\n",
    "    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, focal=fx, pp=(cx, cy))\n",
    "\n",
    "\n",
    "    R = R.transpose()\n",
    "    t = np.matmul(R, t)\n",
    "\n",
    "    if prev_R is None:\n",
    "        curr_R, curr_t = R, t\n",
    "    else:\n",
    "        curr_R, curr_t = np.matmul(prev_R, R), np.matmul(prev_R, t) + prev_t\n",
    "\n",
    "    # draw the current image with keypoints\n",
    "    curr_img_kp = cv2.drawKeypoints(cur_img, kp2, None, color=(0, 255, 0), flags=0)\n",
    "    cv2.imshow('keypoints from current image', curr_img_kp)\n",
    "\n",
    "\n",
    "    # draw estimated trajectory (blue) and gt trajectory (red)\n",
    "    offset_draw = (int(1000/2))\n",
    "    cv2.circle(trajMap, (-int(curr_t[0])+offset_draw, int(curr_t[2])+offset_draw), 1, (255,0,0), 2)\n",
    "    cv2.circle(trajMap, (int(gt_T[i][0, 3])+offset_draw, -int(gt_T[i][2, 3])+offset_draw), 1, (0,0,255), 2)\n",
    "    cv2.imshow('Trajectory', trajMap)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    prev_R, prev_t = curr_R, curr_t\n",
    "    prev_img = cur_img\n",
    "    \n",
    "cv2.imwrite('trajMap.png', trajMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "flownet3d.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e198f1908a66dd117225439470da3cff30e078829bae2c870af17bad0faadbc1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('comp411')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
